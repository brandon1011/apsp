\documentclass[10.5pt,journal]{IEEEtran}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Graph Learning Heuristic for All Pairs Shortest Path}
\author{Brandon Wu, Miodrag Potkonjak}
\maketitle

\begin{abstract}
For an weighted, undirected graph $G = (E,V)$, let distance $\hat{\delta}(u,v)$ be an approximation of the actual
shortest path distance $\delta(u,v)$ between vertices $u$ and $v$, $\forall u,v\in V$. Reasonably good
approximations of the all pairs shortest path (APSP) problem that can be performed with 
real-time performance constraints are valuable in practical route planning applications.\\

In this paper, we present a simple learning framework, using Random Forest Regression, to prune dense graphs
in order to reduce the time and space complexities of computing distance queries between any two vertices in
the graph. Using this scheme, we show that 50\% of edgegs can be pruned while only increasing the average
path cost by 10\%.\\

In general, we introduce a scheme for efficiently compressing graphs while maintaining properties of interest.
\end{abstract}

\section{Introduction}
In graph theory, the class of ``Shortest path" problems, i.e. find a path between any two vertices $u,v$ such
that the sum of the edge weights along this path is minmized are well understood. The variant ``All pairs
shortest path" (APSP) has a polynomial time solution in the Floyd-Warshall Algorithm.
Variant flavors on this algorithm have been proposed in 
\cite{approx1},\cite{approx2},\cite{neuralNet},\cite{astar}
to reduce the run-time by approximating shortest paths. \\

There is much interest today in finding ``approximate" solutions to APSP. For instance, in mapping or route-planning
software, responsivity is often more important to user-experience than optimality. For these applications, it would
be useful to generate a lookup table to answer queries that specify a origin and destination and return a
low cost path between them. In this paper, we study methods for simplifying graph search on APSP by introducing
a heuristic to generate a sparse subgraph $G'$ of the original graph $G$ that significantly reduces run-time
cost while producing a low cost solution.

\subsection{Problem Formulation}
	A weighted, undirected graph is a graph $G=(E,V)$ where each edge $e=(u,v)\in E$ has an associated weight
	$w_e$. A path $p_{u,v}$ between vertices $(u,v)\in V$ is a set of edges that connects $u$ and $v$. The cost
	of the path is the sum of the edge weights, $cost(p) = \sum_{e\in p}w_e$. APSP returns distances $\delta$
	for all pairs of vertices $u,v$ such that $\delta(u,v) = min_{p\in P} (cost(p))$ where $P$ is the set of
	all valid paths between $u$ and $v$.

\subsection{Related Work}
	Algorithms for solving APSP have long been studied in Graph Theory. The classical results from
	Floyd \& Warshall \cite{floydWarshall}
	which has a time complexity $\Theta(|V|^3)$ is often a standard benchmark for undirected, dense graphs.
	Johnson's Algorithm
	\cite{johnson}
	solves APSP faster than Floyd-Warshall on sparse graphs in $\Theta(|V|^2 \log|V|+ |E||V|)$ time,
	using Dijkstra's algorithm for single source shortest pasth as a subroutine. \\
	
	More recent research forcuses on finding ``Approximate Algorithms" for APSP, which declares a
	lower bound on solution quality. A {\it{t-approximate}} algorithm for APSP requires that
	for all $u,v$, $\delta(u,v) \le \hat{\delta}(u,v) \le t\delta(u,v)$.
	\cite{approx1},\cite{approx2},\cite{neuralNet}.
	
	Our approach is orthogonal to that of approximate algorithms: we wish to introduce an algorithm
	that is statiscally close to the exact algorithm, i,e,. $\forall\,u,v \in G,\, 
	\delta(u,v) \le \hat{\delta}(u,v) = p\cdot\delta(u,v)$, where $p$ follows some probability
	distribution such that $\lim_{p\rightarrow\infty}Pr(p) = 0$.
	
\subsection{Our Contributions}
	In this paper, we present a learning approach to graph pathfinding. Heuristic All Pairs Shortest Path (HAPSP)
	finds approximate solutions to APSP on some graph $G$
	using Random Forest regression to predict a useful subgraph $G'$ such that the cost of running an exact
	APSP algorithm on $G'$ is significantly lower than on $G$, while preserving stronger solution quality
	than a {\it{t-approximate}} algorithm. \\
	
	Moreover, our Heuristic APSP scheme may be a useful tool for general graph-based applications: we show
	significant gains by approximating graphs as compressed subgraphs, with acceptable losses in
	optimality. 

\section{Heuristic All Pairs Shortest Path}
The goal of the heuristic is to characterize a graph in such a way to accurately predict
the importance of a particular edge. The importance of an edge is defined in this paper to be the frequency of
its occurences in a shortest path. An edge should be characterized in such a way that is general enough
that it ignores the global topology of the graph it resides in but specific enough to capture its local
importance. By selecting features specific to an edge and those apply to an entire graph, it is possible to
generate a simple linear regression model using standard machine learning techniques to predict
the relative importance of each edge. A subgraph is then constructed by applying a threshold to the importance
prediction on which an optimal algorithm, such as Floyd-Warshall (FW) solves APSP.\\

Our system performs APSP in two phases: (1) an offline phase takes several graph instances and solves them
using FW (exact APSP), the results of which are used to train the regression model, and (2) an online phase
which accepts a new graph $G$ (the input to HAPSP), generates a feature set for $G$, using the predictions
on the regression model to generate a subgraph $G'$, which is then solved using an exact APSP algorithm.

\subsection{Heuristic Edge Pruning Overview}
	The general workflow for the offline and online phases of HAPSP are described below:
	%%%%%%%%%%%%%%%%%%%%%%%%% ALG %%%%%%%%%%%%%%%%%%%%
	\begin{algorithm}
	\caption{General HAPSP Workflow}
	\begin{algorithmic}[1]
	\State let $T\leftarrow$ importance threshold
	\Procedure{GenerateModel}{}
		\State Training set $S\leftarrow$ set auto-generated graphs
		\State For each graph $G_i\in S$ build feature set $f_i$
		\State Regression model $M = $ TRAIN($f_1,...f_n$)
		\State\Return $M$
	\EndProcedure
	\Procedure{BuildSubgraph}{model $M$, $G$=$E,V$}
		\State Initialize $(E',V')$ where $V'=V$ and $E' = \emptyset$
		\State Build feature set $f$ from $G$
		\State Predictions $y = M$.predict($f$)
		\For{each edge $e$ in $E$}
			\If{$y_e>T$}
				\State $E'=E'\cup \{e\}$
			\EndIf
		\EndFor
		\State\Return $(E',V')$
	\EndProcedure
	\Procedure{HAPSP}{graph $G$}
		\State $M\leftarrow$\Call{GenerateModel}{}
		\State $G'\leftarrow$\Call{BuildSubgraph}{$M,G$}
		\State Solve exact APSP on $G'$
	\EndProcedure
	\end{algorithmic}
	\label{fig:alg}
	\end{algorithm}

\subsection{Feature Selection}
	A prediction for a particular edge $e\in E$ in a graph $G=(E,V)$ 
	is an approximation for the number of shortest paths 
	that $e$ occurs in between all pairs of vertices $u,v\in V$ for the solution of APSP on $G$, denoted as 
	COUNT($e$). $G'$ is constructed from edges $e\in E$ where COUNT$(e)>T$ for some threshold $T$. 
	The features used to characterize each edge in $G$ are the following:
	\begin{itemize}
		\item {\bf{Edge weight}}: $w_e$ for each edge $e\in E$. This takes time $\Theta(|E|)$ total time.
		\item{\bf{Max connectivity (MC)}}: for each edge $e=(u,v)$, MC$(e)$=max($|$adj($u)|,|$adj($v)|$).
			Assuming each vertex $v$ has access to adj($v$) in $\Theta(1)$, this step is $\Theta(|E|)$.
		\item{\bf{Min connectivity}}: symmetric to max connectivity, $\Theta(|E|)$. 
		\item{\bf{Min cost neighbor (MCN)}}: MCN($e$=$(u,v)$) = min($w_{e'}$) for all $e'\in$ adj($u),\cup$
			adj($v$). By pre-sorting $E$ ($\Theta(|E|\log|E|)$), lookups of min cost edge for each $v\in V$
			can be done in $\Theta(1)$, so this step is $\Theta(|E|\log|E|)$.
		\item{\bf{Average edge weight (AVG$_w$)}}: for a graph $G$, AVG$_w = \frac{1}{|E|}\sum_{e}w_e$.
			The cost is $\Theta(|E|)$.
		\item{\bf{Number of edges}}: $|E|$, which is $\Theta(1)$.
	\end{itemize}
	Therefore, the cost of the on-line feature generation step is $\Theta(|E|\log|E|)$.
	
\subsection{Regression Model}
	{\bf{To Do: Info about Random Forest}}

\subsection{Shortest Path Algorithm}
	Exact algorithms for APSP are used in HAPSP in two places: (1) to generate labels on the training
	data for building the gression model and (2) on the reduced subgraphs to answer approximate
	distance queries of the form $\delta(u,v)$. There is also need for exact APSP in order to
	compare solution quality on the full graph. \\
	
	For the comparison metrics, we use the standard FW algorithm, although more recent work
	introduces faster algorithms that improve upon the asymptotic time complexity of FW. Such
	is not the focus of the paper, exact APSP is a sub-routine of HAPSP and any algorithm that
	improves upon FW can be used within HAPSP interchangeably; 
	in this paper, FW will serve as a benchmark. The pseudo code for FW is shown in Alg:\\
	%%%%%%%%%%
	\begin{algorithm}
	\caption{Standard Floyd Warshall}
	\begin{algorithmic}[1]
	\State $\forall u,v \in V$, initialize $\delta(u,v)\leftarrow\infty$
	\State $\forall u \in V$, $\delta(u,u)\leftarrow 0$
	\State $\forall e = (u,v)\in E,\,\delta(u,v) = w_e$
	\For{$k \in (1,...,|V|)$}
		\For{$i \in (1,...,|V|)$}
			\For{$ j \in (1,...,|V|)$}
				\State $\delta(i,j) \leftarrow \min(\delta(i,j),\delta(i,k)+\delta(k,j))$
			\EndFor
		\EndFor
	\EndFor
	\end{algorithmic}
	\label{fig:alg2}
	\end{algorithm}
	
	This algorithm is $\Theta(|V|^3)$. Therefore, our edge pruning techniques would not improve upon
	the run-time of the exact APSP if we were to compare FW($G$) for some graph $G$, to FW($G'$),
	where $G'$ is the edge-pruned graph constucted from $G$. In order exploit the reduction in graph
	size, we instead apply a specialized algorithm, such as Johnson's Algorithm, which applies to
	sparse graph. Therefore, if we can construct a representative sparse graph $G'$ of $G$,
	we can improve upon the run-time of exact APSP.\\
	
\section{Experimental Results}
	In this section, we outline the workflow for comparing the performance of HAPSP with the
	standard optimal algorithms. In the first section, we outline the data generation process
	for the graphs used in training the model and testing. Next, we briefly discuss the quality
	of the regression model.
	
	Then we discuss our experimental methods and display our results. This section is followed by
	a summary of the results and discussion.
	\subsection{Graph Generation}
		Throughout the experiment, we use the Random Geometric Graph (RGG) technique for auto-generating graphs
		\cite{randGeom},\cite{randGeom2}.
		The Random Geometric graph is a simple spatial model heavily used in graph theory for its simplicity
		but also geometric properties. An RGG is constructed by taking $N$ vertices and placing them
		uniformly at random in a $k$-dimensional unit space (e.g. if $k=3$ these vertices would exist in
		the unit cube $x_i,y_i,z_i = U(0,1),U(0,1),U(0,1)$ where $U(*)$ is a function that returns a uniform
		distribution on its input argument.\\
		
		A threshold value $T$ is chosen in the range $(0,\sqrt{k})$. For each pair of vertices $(u,v)$ in
		the graph $G$, an edge exists between $u$ and $v$ if the Euclidean distance between them is less
		than $T$. The weight of the edge is the Euclidean distance. Therefore, the a small $T$ produces a
		graph that is more sparse than a larger one. In this report, we have empircally chose $T=0.4$
		since we want a strong guarantee that the graph is connected, but also want $|E| << |V|^2$ to
		avoid a trivial solution to shortest distance queries.\\
		
		The number of vertices is uniformly selected at random in the domain (1000,1200), chosen empirically
		based on the hardware limitations of the experiment machines.
		
	\subsection{Feature Generation}
		Generating a feature set is required on all graphs for HAPSP. We construct our model with the 6
		features described in section {\it{II.B}}. Generation of these features is straight-forward from
		their description (see {\it{II.B}}). \\
		
		For the training set and cross validation data, it is also necessary to generate the data labels.
		That is, to ``count" the actual number of occurences of each edge $e \in E$ in a shortest path.
		This process is done by pre-pending that graph's feature set with a ``Count" field, which is
		computed by executing Dikstra's Algorithm for single source on each $v\in V$ as the source. 
		This computation is very expensive, but it is assumed that the model generation step can be
		performed off-line. Dijkstra's algorithm is preferred to the standard FW because its implementation
		of path reconstruction is more straight-forward than FW.
		
	\subsection{Random Forest Regression Model}
		Our methods for measuring the quality of the random forest regression fit was performed independently
		of the performance benchmarks for our algorithm. We use the popular Python library {\it{scikit-learn}}
		\cite{scikit}
		for generating our Random Forest regression model. We generate the model using 100 trees. The quality
		of the fit is measured by computing the $R^2$ value of the regression:
			\[R^2 = 1 - \frac{SS_{res}}{SS_{total}}\]
			Where $SS_{res}$ and $SS_total$ are defined as:
			\[SS_{res} = \sum_i (y_i-f_i)^2 \,\, SS_{total} = \sum_i (y_i-\bar{y})^2\]
		For each label $y_i$, and its associated prediction $f_i$, where $\hat{y}$ is the average of
		the $y_i$'s. For our analysis, we use {\it{10-Fold Cross Validation}} which is a standard
		learning technique for assessing the quality of a model more accurately by comparing predictions
		to ``new data". We divide the data into 10 equally sized partitions, $p_i$, where $i=1,...,k$, where
		in our case $k=10$ and at each step $j = 0,...,k$, we train our model on all $p_i$, $i\in [k],
		i\ne j$, and measure the residuals by predicting with the data in $p_j$. \\
		
		The results of the 10-Fold Cross Validation are show in Table \ref{fig:crossval_tbl}. \\
		
		\begin{table}[h!] \normalsize
			\caption{Random Forest 10 Fold Cross Validation}
			\centering
			\begin{tabular}{|l|l|l|l|l|} \hline
			Fold    & $R^2$ & $|$Res$|$ & Avg Error    & Avg \% Error \\ \hline 
			1       & 0.57060        & 4700.390  & 11.554 	& 0.6516      \\
			2       & 0.56713        & 4727.749  & 11.659 	& 0.6802        \\
			3       & 0.55107        & 4682.334  & 11.501  	& 0.7050       \\
			4       & 0.55022        & 4814.936  & 11.916 	& 0.7047        \\
			5       & 0.55189        & 4738.939  & 11.690 	& 0.6812        \\
			6       & 0.57748        & 4640.270 & 11.405 	& 0.6817        \\
			7       & 0.56993        & 4613.565  & 11.483 	& 0.6929        \\
			8       & 0.53312        & 4795.335  & 11.730 	& 0.7226        \\
			9       & 0.51987        & 5068.946  & 12.527  	& 0.6928       \\
			10      & 0.56030        & 4805.192  & 12.052  	& 0.7402        \\ \hline
			Avg		& 0.55516        & 4758.766 & 11.7522 	& 0.6953        \\ \hline
			\end{tabular}
			\label{fig:crossval_tbl}
		\end{table}
		
		The average $R^2$ value across all of the folds is $0.555$, which is not nearly the ideal
		\textasciitilde 1. In addition, the average prediction error is 11.75, which is the average
		difference of the real number of occurences in a shortest path measured by the optimal algorithm
		and the predicted number of occurences. This number is significant since it is on the order
		of the thresholds with which we use to decide whether to prune an edge (the threshold used in
		experiments is 25). Moreover, the percent error which is:
			\[\mbox{Percent Error} = \frac{|y_i - f_i|}{y_i}\]
		is on average 69.5\%. \\
		
		This data suggests there is much room for improvement on this model.
		Intuitively, it is hard to characterize the importance of a particular edge, disregarding the
		specific topology of the graphs the model is trained on (in order to maintain generality for
		new graphs). Additionally, time complexity is a limitation to the quality of the feature set since it is
		important that features be generated efficiently, given that the Feature Generation step
		is a part of the online phase of the HAPSP algorithm.
	
	\subsection{Performance Metrics}
		The following sections summarizes the main results of this paper. Using the Regression model described
		in section {\it{III.C}}, we apply HAPSP on newly generated random geometric graphs. 
		The primary metric of interest in the percent loss of optimality. For, instance, the 
		approximate shortest distance $\hat{\delta}(u,v)$ between two vertices $u,v$ returned by
		HAPSP is {\it{p-suboptimal}} with respect to the actual shortest distance $\delta(u,v)$
		by the relation:
			\[p=\frac{\hat{\delta}(u,v) - \delta(u,v)}{\delta(u,v)}\]
			
		This metric quantifies the percentage increase in the cost of the path returned by HAPSP versus
		the optimal algorithm. We are interested in the average and worst case numbers. \\
		
		In addition, it is useful to quantify graph compression ratio $R$, which measures the 
		number of edges elminated in the HAPSP algorithm:
			\[R = 1 - \frac{|E'|}{|E|}\]
		where $E'$ is the result of running the BUILDSUBGRAPH procedure on G, described in
		Algorithm \ref{fig:alg}. Intuitively, $R$ and $p$ should be inversely related. If we
		decide to more aggressively prune our graphs, then we should expect that our solution quality
		degrades further. However, it is important to attain a large $R$, since this decreases
		the runtime of HAPSP. We aim to maximize $R$ (thereby decreasing runtime),
		such that we suffer acceptable degredations of solution quality. Both quantities are empirical
		and application specific. \\
		
		We performed our tests on Intel i7 hardware. 
		
	\subsection{Main Results}
		
		We perform 100 test instances with new randomly generated graphs. For each instance, we
		compute the optimal APSP. Then, we generate a feature set for the graph and prune the graph
		according the the SUBGRAPH procedure in Algorithm \ref{fig:alg}. Finally, we again compute
		the optimal APSP on the subgraph. \\
		
		The results of this experiment are shown in Table \ref{fig:main_tbl}.
		
		\begin{table}[h!] \normalsize
		\centering
		\caption{Experimental Results of HAPSP vs FW}
		\label{my-label}
 		\begin{tabular}{|l|l|} 								\hline
		$|E|_{\mbox{avg}}$              & 81754.39			\\\hline
		$|V|_{\mbox{avg}}$              & 1099.99			\\\hline
		$R_{\mbox{avg}}$                & 0.50014945		\\\hline
		Avg Error         				& 0.029065182		\\\hline
		Avg \% Error 					& 0.097277609		\\\hline
		Avg Max \% Error      			& 47.666938			\\\hline
		Max \% Error			        & 225.251			\\\hline
		\end{tabular}
		\label{fig:main_tbl}
		\end{table}
	
		The importance threshold $T$, for edge pruning was chosen empirically, with the intent of
		maximizing $R$ while maintaining an acceptable loss in optimality. For this experiment,
		$T$ varied between 20 and 25, depending on $|V|$. 
		$R_{\mbox{avg}}$, the average graph compression ratio across all test instances was 50\%, while
		the average solution cost suffered an increase of 9.73\%. More aggressive edge pruning led to
		a steep increase in the average percent error. \\
		
		Another important metric is the maximum percent error, which quantifies the worst case scenario
		for distance queries to HAPSP. The average maximum percent error was computed by computing the
		max percent error for each test instance, and averaging across all 100 instances. The maximum
		percent error was computed by taking the maximum percent error across all test instances. \\
		
		For many practical applications, the average percent error of \textasciitilde10\% is acceptable,
		although the maximum percent error is very large (4,766\% on average and 22,525\% worst case).
		A likely cause is our choice of interpretation on edge importance: we consider edges that only
		{\it{frequenty}} appear in shortest paths as important. Consider an edge that connects two
		vertices $e=(u,v)$ in a graph $G$. It may be the case that $e$ appears in very few shortest paths,
		but significantly contributes to those paths. Also, our model fails to account for dependence of
		edges on other edges: i.e., deleting an edge affects the relative importance of other edges.
		In these scenarios, our scheme for graph reduction is insuffiient. \\
		
		However, if distance queries of this type are low probability events, practical systems can
		still find utility in our HAPSP scheme. \\
		
		\begin{figure*}[] \centering
		\includegraphics[width=5in]{../plots/errorplot_1.png} 
		\caption{Solution quality of HAPSP vs APSP}
		\label{fig:error_plt}
		\end{figure*}
		
		Fig \ref{fig:error_plt} summarizes the frequency of percent error across all test instances. 
		For each distance test ($\forall\, u,v\in V,\,\mbox{error} = \hat{\delta}(u,v) - \delta(u,v)$),
		for each test instance $i$, $i = 1,...,100$, the percent error is binned by computing:
			\[\lceil\frac{\hat{\delta}(u,v) - \delta(u,v)}{ \delta(u,v)}\rceil\]
		Note that errors $< 0.025\%$ are rounded down and errors $\ge 20\%$ are binned to $20\%$.\\
		
		The data in Fig. \ref{fig:error_plt} show that a majority of distance queries are very close
		to the optimal solution. 40\% of path queries increased by less than 0.025\% while 62\% of
		queries increased by less than 2\%. \\
		
		Interestingly, the frequency of errors appears to follow an expoentially decreasing function,
		which implies that large errors are highly unlikely events. The aggregate frequency for all
		errors $>=$20\% is 10\%, which means that 90\% of all distance queries result in an cost
		increase of $<20\%$.
	
	\subsection{Discussion}
		Our results show that the Heuristic method for compressing graphs is a reasonable approximation
		of the original graph. The average case loss in optimality is $<10\%$, with a majority of
		queries with a loss of less than $<2\%$. We have also shown that the probability distribution
		of errors follows a decaying exponential, such that intolerable errors are very low probability
		events.\\
		
		Therefore, the graph compression techniques we introduce may be useful in real-time
		applications, where low-probability outliers can be discarded, since it effectively
		generates subgraphs which are often nearly optimal, but also much cheaper in terms
		of run-time cost of graph search algorithms on these subgraphs. \\
		
		However, our methods fail to demonstrate the ability of HAPSP to compress a graph 
		significantly: the performance of our heuristic sharply declines if we prune more aggressively.
		By reducing the size of the graph by 50$\%$, the subgraph we generate is still not a ``sparse"
		graph, that is, $|E| \sim |V|$, so we cannot argue significant savings in the run-time cost of
		computing shortest path distance queries. \\
		
		The methods we develop in this paper are widely applicable to all families of graph search
		problems and are not limited to SP queries. The primary result of the paper is that we
		show that Learning Heuristics using popular regression models can yield reasonable
		approximations on dense graphs, in which the compressed graph retains properties of
		particular interest in the original graph. In the case of HAPSP, we retain distances between
		arbitarily vertices, with a high degree of fidelity. Graph compression frameworks may be
		useful to other families of problems in graph theory.
	\subsection{Improvements \& Future Work}
		There is much room for improvement within the HAPSP framework. Increasing the accuracy of the
		regression model would allow graphs to be pruned more aggressively, since it would more
		accurately identify edges that contribute in shortest paths. The challenge is to determine
		important edge features that are efficiently computable. Additionally, it is possible to
		find a more valuable heuristic than the number of occurences in a shortest path, since
		the number of occurence in the shortest path do not quantify the contribution of that edge
		to that path, and whether that path would have suitable alternate choices, should the edge
		be deleted. Similarly, this heuristic fails to account for the dependence of edges on
		other edges. One edge $e$, might contribute to many shortest paths in $G$ due to some
		other edge $e'$, but $e$ might be useless in the graph $G\setminus e'$. These nuances
		are difficult to capture with standard machine learning techniques and might fundamentally
		limit the effectiveness of learning based graph pruning. \\
		
		Another major shortcoming of the results of this paper is that there is no work performed
		on different families of graphs. It may be the case that the Regression model was only
		successful on Random Geometric Graphs, and would perform poorly on different classes of graphs,
		for example
		on graphs where edges are generated on a uniform distribution. More importantly,
		we did not attempt to solve real-world graphs, such as graphs modelling road maps.
		
\section{Acknowledgements}
	Special thanks to Miodrag Potkonjak for his extensive involvement and helpful discussions
	on this project. For more detailed information about the implementation, please
	refer to the code repository on Github: https://github.com/brandon1011/apsp. 
	\begin{thebibliography} {2} \normalsize
	
		 \bibitem{approx2} Baswana, and S. Kavitha, T.
         Faster Algorithms for All-Pairs Approximate Shortest Path in Undirected Graphs.
         Society for Industrial and Applied Mathematics,2010.39:7.p.2865-2896.
         
		\bibitem{randGeom2} Dall, J. and Christensen, M.
		Random Geometric Graphs.
		Physical Review E 66 (2002).
	
		\bibitem{astar} Goldberg, A. and Harrelson, C.
		Computing Shortest Path: A* Search Meets Graph Theory.
		Society for Industrial and Applied Mathematics, 2005.
		p.156-165.
		
		\bibitem{johnson}
		Johnson, Donald B.
		Efficient algorithms for shortest paths in sparse networks.
		Journal of the ACM 24 (1),1977:p.1â€“13.
		
		\bibitem{floydWarshall}
		Ingerman, Peter Z.
		Algorithm 141: Path Matrix. 
		Communications of the ACM 5 (11),1962.
		p.556.

		\bibitem{approx1} Kanai, T. and Suzuki, H.
		Approximate shortest path on a polyhedral surface and its applications.
		Computer-Aided Design 33 (2001).p.801-811.
		
		\bibitem{randGeom} Muthukrishnan, S. and Pandurangan G.
		The Bin-Covering Technique for Thresholding Random Geometric Graph Properties.
		Society for Industrial and Applied Mathematics, 2005.
		p.989-998.
		 
		\bibitem{scikit} Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.
         Scikit-learn: Machine Learning in Python.
         Journal of Machine Learning Research, 2011.12.p.2825-2830.
 
         
         \bibitem{neuralNet} Wang, X. and Qu, H. and Yi, Z.
         A modified pulse coupled neural network for shortest-path problem.
         Neurocomputing 72 (2009).
         p.3028-3033.
         

	\end{thebibliography}
\end{document}